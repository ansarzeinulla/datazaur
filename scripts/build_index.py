import json
import os
import shutil
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tqdm import tqdm # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –Ω—É–∂–µ–Ω, —á—Ç–æ–±—ã –Ω–µ —Å–∫—É—á–∞—Ç—å

CORPUS_DIR = Path("data/corpus")
DB_DIR = "chroma_db"
BATCH_SIZE = 200 # –ü–∏—à–µ–º –ø–∞—á–∫–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏—Ç—å –ø–∞–º—è—Ç—å

def build_vector_db():
    # 0. –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—É—é –±–∞–∑—É
    if os.path.exists(DB_DIR):
        shutil.rmtree(DB_DIR)

    print("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (rubert-tiny2)...")
    embeddings = HuggingFaceEmbeddings(model_name="cointegrated/rubert-tiny2")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º 500/100 (–ª—É—á—à–µ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏, —á–µ–º 1000)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " "]
    )

    documents = []
    metadatas = []

    print(f"üìÇ –°–∫–∞–Ω–∏—Ä—É–µ–º {CORPUS_DIR}...")
    json_files = list(CORPUS_DIR.glob("*.json"))

    # 1. –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã (–≤ –æ–¥–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –∑–∞—Ç–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ)
    for file_path in tqdm(json_files, desc="–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤"):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                
            items = data if isinstance(data, list) else [data]
                
            for item in items:
                text = item.get("text", "")
                if not text: continue
                    
                # –î–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                title = item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                codes = ", ".join(item.get('icd_codes', []))
                
                chunks = text_splitter.split_text(text)
                
                for chunk in chunks:
                    # –•–ê–ö –î–õ–Ø –ü–û–ë–ï–î–´: –í—à–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä—è–º–æ –≤ —Ç–µ–∫—Å—Ç
                    enriched_text = f"–ë–û–õ–ï–ó–ù–¨: {title}. –ö–û–î –ú–ö–ë: {codes}. –¢–ï–ö–°–¢: {chunk}"
                    
                    documents.append(enriched_text)
                    metadatas.append({
                        "protocol_id": item.get("protocol_id", ""),
                        "title": title,
                        "icd_codes": codes
                    })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å —Ñ–∞–π–ª–æ–º {file_path}: {e}")

    total_chunks = len(documents)
    print(f"üß© –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø–∏—Å–∏: {total_chunks} —á–∞–Ω–∫–æ–≤.")

    # 2. –ü–∏—à–µ–º –≤ –±–∞–∑—É
    print("üß† –°–æ–∑–¥–∞–µ–º –±–∞–∑—É ChromaDB...")
    vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    
    # –ü–∏—à–µ–º –±–∞—Ç—á–∞–º–∏ (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±)
    for i in tqdm(range(0, total_chunks, BATCH_SIZE), desc="–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"):
        batch_docs = documents[i : i + BATCH_SIZE]
        batch_meta = metadatas[i : i + BATCH_SIZE]
        vector_db.add_texts(texts=batch_docs, metadatas=batch_meta)
    
    print(f"‚úÖ –ë–∞–∑–∞ –≥–æ—Ç–æ–≤–∞: {DB_DIR}")

if __name__ == "__main__":
    build_vector_db()